// Todo list and general notes.

REMEMBER!: tsconfig.json has sections that control the approved image download URLs!  We will need to add our AWS S3 bucket to this list.

TRIAGE: Save the LoRA implementation for a micro-grant.  Don't try
 to add it to the contest entry.  Save that time for Zora NFT
 minting.

IMPORTANT!: Take heed this comment for the Livepeer category we
    are targeting in the AI Video hackathon!:

    Create a fun and engaging web app that utilizes Livepeer AI pipelines to provide a novel user experience with viral potential. High-quality entries will be easy to use and share with others.

    I think it will be enough to add a typical "share bar".  See if
     one exists already for React/Next.

FUTURE FEATURES:

- Incorporate the augmentation parameter some stable diffusion models support
- Incorporate LoRA model usage (micro-grant)
- Detect new image creation session.  If that happens, reset the current state object for the chat session.

NEXT:

    // need websocket client/server request/response code to read/write
    //  user blockchain presence objects.

    LATER: Add server side verification of user public address via
     client side payload signature checking.

    > Big bug: Major items from the prompt are being lost
        during refinement attempts.

    Interesting enhancement:  Modify the prompt reordering rewrite
     code to force the primary error content to the front of the
     prompt by isolating the component image elements and doing this
     explicitly.

NEW STREAMLINED APPROACH:

I think the rewrite feature is triggering a JSON error.  When I make specific complaint about the image being wrong, I see that error.

- Change the color of the error text so it is more visible.

- The prompt rewrite feature is not being triggered reliably.

> These phrases did not trigger the wrong_content detector:

    + The elephant is not dancing.

    + Only want one Luke.  I can see two of them in the image.

- Almost done with full chat volley pipeline:

    NEED DEFAULT NEGATIVE PROMPT!

- Do we need to store the images in an S3 bucket for the contest?

- The wrong_content detector needs improvement.  Example:

    Previous prompt:

    Generate an image of several monks dressed in golden frocks surrounding a monolith as they fervently pray for the Giant Chicken of Ragnarok to manifest. Ensure the scene exudes solemnity and reverence, with deep concentration depicted on the monks' faces. The setting should convey a mystical atmosphere, possibly with subtle ethereal lighting to enhance the spiritual ambiance. Include a majestic manifestation of the Giant Chicken of Ragnarok in a prominent position within the scene. The monks should have their hands raised in exhortation.

    User feedback:

    "The monks hands are not raised."

    Probably need a dedicated wrong_content detector that gets passed the last prompt and the user input, and looks for specific instances of the user input that are semantically linked but in negation to some part of the previous prompt.

    In this case:

    neg: "The monks should have their hands raised in exhortation"

        + Also, make sure this is why the prompt rewrite feature did not kick in.


TESTS TO RUN:

- Need to figure out the sweet spot in CFG and STEPS for the FLUX model to get clear, legible text.  Use the prompt below and try the major CFG STEPS value pairs, writing the generated image to a file that bears the CFG and STEPS values in the primary file name.  Have GPT write the code.

"A large polar bear in profile, looking down at a smaller seal, while the seal lies on a sandy beach making direct eye contact with the camera. Both are far from the ocean, with a vast stretch of beach behind them. The scene has a surreal, dreamlike atmosphere, with soft pastel-colored skies, and the ocean in the far background. A wooden sign nearby reads "Don't feed the seals!" in large easy to see letters. The polar bear is towering over the seal, with exaggerated size contrast, photorealistic details, soft shadows, and a vibrant, painterly style"

FEATURES FOR AFTER CONTEST:

* We currently do detect when the user wants text on an image and then switch to the Flux model in that case.  But we don't have code to detect when they DON'T want text on an image anymore and to switch to another model, or at least no longer constrain the model choice to flux.

* Tag each image in the Carousel with the chat volley ID it is associated with, the one that generated it.  Then, when the scroll through the images, change the response window to the response belonging to the image.  If they enter a prompt, pass in the chat history using the associated chat volley (so use that index and BEFORE to build the chat history, not from the end of the chat history).

NEEDED INFRASTRUCTURE:

    - Maintain per-session array:

        * Array of user/llm texts detail objects:

         {
            role: [user | system ],
            text_said: string,
            decorator_object: [user object | null]
         }

    - Maintain a log file with the session ID as name.  Append
        each object like that above as it is created during
        a session.

NEEDED FROM LLM:

    UPDATE: Conversation modes are discarded.  There is only
     one conversation mode.  Also, we are no longer going
     to try and use the LLM to make any parameter or
     model changes, only text prompt changes.  Instead,
     we will use code to interpret the results from the
     INTENT DETECTORS and make the appropriate changes
     that way.

     UPDATE-2: We are switching to a simple mint to Zora and
      not pursuing Story Protocol any more.  Story Protocol
      is a fascinating and powerful platform, but there is
      not enough time in the contest to create a sophisticated
      front-end to their IP management features.  Zora will
      be mint and go.

      However, we will need some form of primitive protection
       or authorization to keep the service from being abused.
       After the contest, we can make it "approved addresses
       only"

    ***** Create separate API key for plastic educator with
     its own budget

    >>>>> We will fire these off in a bulk Promise request since they
     are not serially dependent, and then do the final steps to
     execute an image generation with the new image generation prompt,
     and execute/store any new model or parameter changes for future
     session volleys.

    >>>>> Don't forget to grab the Hugging face negative prompts for
     Lightning and Flux and incorporate them into the prompt we send
     to Livepeer

    NEED TO FIGURE OUT:

    * The actual image generation text prompt to send to Livepeer for image generation and to be forwarded to the client front-end for display

    * Suggestions for changes to
        - The current image generation model
        - The CFG parameter
        - The steps parameter

    * The reply to the user:

        BRAINSTORMING mode:
        REFINE mode:

    * An explanation of any changes made to (what and why):

        - The current image generation model
        - The CFG parameter
        - The steps parameter

    NEXT: Implement above and implement the code that reacts to model
     or parameter change suggestions from the LLM.

    * Create log file of full session with user.  Assign a per session ID
     to a thread.  Implement the logging.  Write the session interaction
     to a text file.

- NEXT: Where and when do we actually make an image request?  The Chatbot
 app flow is more complicated than the Storytime app.  Call handleImageRequest()
 once that is determined.

- Lower stable diffusion steps during BRAINSTORM mode and higher in REFINE mode.
- Add the general negative prompts from storytime repo
- Use sharp NPM package to optimize images before returning them to the client
- Copy the optimized images to our S3 bucket
- Use S3 bucket URLs for NFT asset URLs.
- Create Amazon S3 bucket for generated images for the contest.  Later we can use a decentralized solution.
-