// Todo list and general notes.

REMEMBER!: tsconfig.json has sections that control the approved image download URLs!  We will need to add our AWS S3 bucket to this list.

TRIAGE: Save the LoRA implementation for a micro-grant.  Don't try
 to add it to the contest entry.  Save that time for Zora NFT
 minting.

IMPORTANT!: Take heed this comment for the Livepeer category we
    are targeting in the AI Video hackathon!:

    Create a fun and engaging web app that utilizes Livepeer AI pipelines to provide a novel user experience with viral potential. High-quality entries will be easy to use and share with others.

    I think it will be enough to add a typical "share bar".  See if
     one exists already for React/Next.

FUTURE FEATURES:

- Incorporate the augmentation parameter some stable diffusion models support
- Incorporate LoRA model usage (micro-grant)
- Detect new image creation session.  If that happens, reset the current state object for the chat session.

NEXT:

    Interesting enhancement:  Create an LLM block to reorder the clauses
     in a previous prompt if the user mentions it in a wrong content
     complaint.  Try the main system prompt first with a template variable
     named ${reorderClauses}.

NEW STREAMLINED APPROACH:

Rewrite this image generation prompt so that the aliens and <wrong_content: matched text> are the most important part of the scene.

${last prompt}

OLD CONVOLUTED ATTEMPT:
     Something like:

You are an image generation prompt builder helper. You excel at helping a user fix problems with their prompts. When the user complains that the image content is wrong, you help them fix it.

When that happens, and if there are multiple element descriptions in the previous prompt, you should:

Correct the content error according to the user's complaint.
Reorder the element descriptions so that the element(s) they complained about are moved to the very beginning of the image prompt, making it the first noun mentioned. It's acceptable to rephrase or combine sentences to achieve a natural flow.
For example, if the previous prompt was:

"A large friendly dog with red fur stands in front of a tall building. On top of the building is a sad bald eagle wearing a coat. The sky is dark."

And the user's complaint was:

"No, I wanted a happy bald eagle."

Then the revised prompt would be:

"A happy bald eagle wearing a coat is on top of a tall building that has a large friendly dog with red fur standing in front of it. The sky is dark."

Make sure the revised prompt retains the same meaning as the original sentences, so do not reorder words that belong together. Ensure the prompt flows naturally, even if rephrasing is necessary.

Here is the last prompt you created based on the user's input:

"A large fly is on a man's arm. His arm is hairy and he wears a silver wrist watch. Under the man's arm is a dog."

Here is the latest user complaint, what is your new prompt?

<user complaint with the matched text>


    NOTE: Need to update the field "lora" to "loras" and make it an object
     where each property is a lora model ID with its value being set to
     the correct version number, and then maintain that field.

- Almost done with full chat volley pipeline:

    NEED DEFAULT NEGATIVE PROMPT!

- Do we need to store the images in an S3 bucket for the contest?

TESTS TO RUN:

- Need to figure out the sweet spot in CFG and STEPS for the FLUX model to get clear, legible text.  Use the prompt below and try the major CFG STEPS value pairs, writing the generated image to a file that bears the CFG and STEPS values in the primary file name.  Have GPT write the code.

"A large polar bear in profile, looking down at a smaller seal, while the seal lies on a sandy beach making direct eye contact with the camera. Both are far from the ocean, with a vast stretch of beach behind them. The scene has a surreal, dreamlike atmosphere, with soft pastel-colored skies, and the ocean in the far background. A wooden sign nearby reads "Don't feed the seals!" in large easy to see letters. The polar bear is towering over the seal, with exaggerated size contrast, photorealistic details, soft shadows, and a vibrant, painterly style"

FEATURES FOR AFTER CONTEST:

* We currently do detect when the user wants text on an image and then switch to the Flux model in that case.  But we don't have code to detect when they DON'T want text on an image anymore and to switch to another model, or at least no longer constrain the model choice to flux.

* Tag each image in the Carousel with the chat volley ID it is associated with, the one that generated it.  Then, when the scroll through the images, change the response window to the response belonging to the image.  If they enter a prompt, pass in the chat history using the associated chat volley (so use that index and BEFORE to build the chat history, not from the end of the chat history).

NEEDED INFRASTRUCTURE:

    - Maintain per-session array:

        * Array of user/llm texts detail objects:

         {
            role: [user | system ],
            text_said: string,
            decorator_object: [user object | null]
         }

    - Maintain a log file with the session ID as name.  Append
        each object like that above as it is created during
        a session.

NEEDED FROM LLM:

    UPDATE: Conversation modes are discarded.  There is only
     one conversation mode.  Also, we are no longer going
     to try and use the LLM to make any parameter or
     model changes, only text prompt changes.  Instead,
     we will use code to interpret the results from the
     INTENT DETECTORS and make the appropriate changes
     that way.

     UPDATE-2: We are switching to a simple mint to Zora and
      not pursuing Story Protocol any more.  Story Protocol
      is a fascinating and powerful platform, but there is
      not enough time in the contest to create a sophisticated
      front-end to their IP management features.  Zora will
      be mint and go.

      However, we will need some form of primitive protection
       or authorization to keep the service from being abused.
       After the contest, we can make it "approved addresses
       only"

    ***** Create separate API key for plastic educator with
     its own budget

    >>>>> We will fire these off in a bulk Promise request since they
     are not serially dependent, and then do the final steps to
     execute an image generation with the new image generation prompt,
     and execute/store any new model or parameter changes for future
     session volleys.

    >>>>> Don't forget to grab the Hugging face negative prompts for
     Lightning and Flux and incorporate them into the prompt we send
     to Livepeer

    NEED TO FIGURE OUT:

    * The actual image generation text prompt to send to Livepeer for image generation and to be forwarded to the client front-end for display

    * Suggestions for changes to
        - The current image generation model
        - The CFG parameter
        - The steps parameter

    * The reply to the user:

        BRAINSTORMING mode:
        REFINE mode:

    * An explanation of any changes made to (what and why):

        - The current image generation model
        - The CFG parameter
        - The steps parameter

    NEXT: Implement above and implement the code that reacts to model
     or parameter change suggestions from the LLM.

    * Create log file of full session with user.  Assign a per session ID
     to a thread.  Implement the logging.  Write the session interaction
     to a text file.

- NEXT: Where and when do we actually make an image request?  The Chatbot
 app flow is more complicated than the Storytime app.  Call handleImageRequest()
 once that is determined.

- Lower stable diffusion steps during BRAINSTORM mode and higher in REFINE mode.
- Add the general negative prompts from storytime repo
- Use sharp NPM package to optimize images before returning them to the client
- Copy the optimized images to our S3 bucket
- Use S3 bucket URLs for NFT asset URLs.
- Create Amazon S3 bucket for generated images for the contest.  Later we can use a decentralized solution.
-