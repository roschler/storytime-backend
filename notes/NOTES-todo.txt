// Todo list and general notes.

REMEMBER!: tsconfig.json has sections that control the approved image download URLs!  We will need to add our AWS S3 bucket to this list.

TRIAGE: Save the LoRA implementation for a micro-grant.  Don't try
 to add it to the contest entry.  Save that time for Zora NFT
 minting.

IMPORTANT!: Take heed this comment for the Livepeer category we
    are targeting in the AI Video hackathon!:

    Create a fun and engaging web app that utilizes Livepeer AI pipelines to provide a novel user experience with viral potential. High-quality entries will be easy to use and share with others.

    I think it will be enough to add a typical "share bar".  See if
     one exists already for React/Next.

FUTURE FEATURES:

- Incorporate the augmentation parameter some stable diffusion models support
- Incorporate LoRA model usage (micro-grant)
- Detect new image creation session.  If that happens, reset the current state object for the chat session.

NEXT:

    - CAN'T USE STREAMING LLM COMPLETIONS. We are not piping the
      text received from the LLm to the client as it comes in
      like Storytime does because expects a story.  Instead, we
      receive the improved prompt text from the LLM and send
      it to the client after post-processing it.  However, we
      do still make the un-awaited async call to create an
      image.

    > Use non-streamed text completions.

    NOTE: Need to update the field "lora" to "loras" and make it an object
     where each property is a lora model ID with its value being set to
     the correct version number, and then maintain that field.

- Almost done with full chat volley pipeline:

    !!!!! Need a chat history summarizer.  See this ChatGPTPlus thread:

        https://chatgpt.com/c/6704baea-e6d8-8008-9c29-3eeb6f4b2e0e

    New strategy:

        Using this JSON object output by the chat history summarizer:

        {
            "final_image_description": "A bustling futuristic city with towering skyscrapers, neon lights, and a vibrant crowd of people. Flying cars, painted in bright yellow, zoom through the city sky.",
            "modification_history": [
                {"modification": "Add flying cars", "status": "applied"},
                {"modification": "Make the cars green", "status": "discarded because it was superseded"},
                {"modification": "Change the cars to yellow", "status": "applied"}
            ]
        }

        Pass this in each iteration and have the LLM update it, instead of
         passing in the chat history.

    NEED DEFAULT NEGATIVE PROMPT!

    FUN PROBLEM: Currently the intent detectors can lead to conflicting
     adjustments, like when the user says the image boring but also
     reports a "wrong_content" complaint.  The former decreases
     guidance_scale and the latter increases it.

    1) Write a new chat volley object immediately, do not
        wait for the images to be generated.

    2) Integrate the test harness code into the websocket
      request flow.

    3) Update the client to handle the new payloads from the
      back-end server, flip the user input and system response
      text areas vertically, make sure we have a nice busy
      animation while waiting for image requests

- Do we need to store the images in an S3 bucket for the contest?

TESTS TO RUN:

- Need to figure out the sweet spot in CFG and STEPS for the FLUX model to get clear, legible text.  Use the prompt below and try the major CFG STEPS value pairs, writing the generated image to a file that bears the CFG and STEPS values in the primary file name.  Have GPT write the code.

"A large polar bear in profile, looking down at a smaller seal, while the seal lies on a sandy beach making direct eye contact with the camera. Both are far from the ocean, with a vast stretch of beach behind them. The scene has a surreal, dreamlike atmosphere, with soft pastel-colored skies, and the ocean in the far background. A wooden sign nearby reads "Don't feed the seals!" in large easy to see letters. The polar bear is towering over the seal, with exaggerated size contrast, photorealistic details, soft shadows, and a vibrant, painterly style"

FEATURES FOR AFTER CONTEST:

* We currently do detect when the user wants text on an image and then switch to the Flux model in that case.  But we don't have code to detect when they DON'T want text on an image anymore and to switch to another model, or at least no longer constrain the model choice to flux.

* Tag each image in the Carousel with the chat volley ID it is associated with, the one that generated it.  Then, when the scroll through the images, change the response window to the response belonging to the image.  If they enter a prompt, pass in the chat history using the associated chat volley (so use that index and BEFORE to build the chat history, not from the end of the chat history).

NEEDED INFRASTRUCTURE:

    - Maintain per-session array:

        * Array of user/llm texts detail objects:

         {
            role: [user | system ],
            text_said: string,
            decorator_object: [user object | null]
         }

    - Maintain a log file with the session ID as name.  Append
        each object like that above as it is created during
        a session.

NEEDED FROM LLM:

    UPDATE: Conversation modes are discarded.  There is only
     one conversation mode.  Also, we are no longer going
     to try and use the LLM to make any parameter or
     model changes, only text prompt changes.  Instead,
     we will use code to interpret the results from the
     INTENT DETECTORS and make the appropriate changes
     that way.

     UPDATE-2: We are switching to a simple mint to Zora and
      not pursuing Story Protocol any more.  Story Protocol
      is a fascinating and powerful platform, but there is
      not enough time in the contest to create a sophisticated
      front-end to their IP management features.  Zora will
      be mint and go.

      However, we will need some form of primitive protection
       or authorization to keep the service from being abused.
       After the contest, we can make it "approved addresses
       only"

    ***** Create separate API key for plastic educator with
     its own budget

    >>>>> We will fire these off in a bulk Promise request since they
     are not serially dependent, and then do the final steps to
     execute an image generation with the new image generation prompt,
     and execute/store any new model or parameter changes for future
     session volleys.

    >>>>> Don't forget to grab the Hugging face negative prompts for
     Lightning and Flux and incorporate them into the prompt we send
     to Livepeer

    NEED TO FIGURE OUT:

    * The actual image generation text prompt to send to Livepeer for image generation and to be forwarded to the client front-end for display

    * Suggestions for changes to
        - The current image generation model
        - The CFG parameter
        - The steps parameter

    * The reply to the user:

        BRAINSTORMING mode:
        REFINE mode:

    * An explanation of any changes made to (what and why):

        - The current image generation model
        - The CFG parameter
        - The steps parameter

    NEXT: Implement above and implement the code that reacts to model
     or parameter change suggestions from the LLM.

    * Create log file of full session with user.  Assign a per session ID
     to a thread.  Implement the logging.  Write the session interaction
     to a text file.

- NEXT: Where and when do we actually make an image request?  The Chatbot
 app flow is more complicated than the Storytime app.  Call handleImageRequest()
 once that is determined.

- Lower stable diffusion steps during BRAINSTORM mode and higher in REFINE mode.
- Add the general negative prompts from storytime repo
- Use sharp NPM package to optimize images before returning them to the client
- Copy the optimized images to our S3 bucket
- Use S3 bucket URLs for NFT asset URLs.
- Create Amazon S3 bucket for generated images for the contest.  Later we can use a decentralized solution.
-