// Todo list and general notes.

REMEMBER!: tsconfig.json has sections that control the approved image download URLs!  We will need to add our AWS S3 bucket to this list.

IMPORTANT!: Take heed this comment for the Livepeer category we
    are targeting in the AI Video hackathon!:

    Create a fun and engaging web app that utilizes Livepeer AI pipelines to provide a novel user experience with viral potential. High-quality entries will be easy to use and share with others.

    I think it will be enough to add a typical "share bar".  See if
     one exists already for React/Next.

NEEDED INFRASTRUCTURE:

    - Maintain per-session array:

        * Array of user/llm texts detail objects:

         {
            role: [user | system ],
            text_said: string,
            decorator_object: [user object | null]
         }

    - Maintain a log file with the session ID as name.  Append
        each object like that above as it is created during
        a session.

NEEDED FROM LLM:

    UPDATE: Conversation modes are discarded.  There is only
     one conversation mode.  Also, we are no longer going
     to try and use the LLM to make any parameter or
     model changes, only text prompt changes.  Instead,
     we will use code to interpret the results from the
     INTENT DETECTORS and make the appropriate changes
     that way.

     UPDATE-2: We are switching to a simple mint to Zora and
      not pursuing Story Protocol any more.  Story Protocol
      is a fascinating and powerful platform, but there is
      not enough time in the contest to create a sophisticated
      front-end to their IP management features.  Zora will
      be mint and go.

      However, we will need some form of primitive protection
       or authorization to keep the service from being abused.
       After the contest, we can make it "approved addresses
       only"

    ***** Create separate API key for plastic educator with
     its own budget

    >>>>> We will fire these off in a bulk Promise request since they
     are not serially dependent, and then do the final steps to
     execute an image generation with the new image generation prompt,
     and execute/store any new model or parameter changes for future
     session volleys.

    >>>>> Don't forget to grab the Hugging face negative prompts for
     Lightning and Flux and incorporate them into the prompt we send
     to Livepeer

    NEED TO FIGURE OUT:

    * The actual image generation text prompt to send to Livepeer for image generation and to be forwarded to the client front-end for display

    * Suggestions for changes to
        - The current image generation model
        - The CFG parameter
        - The steps parameter

    * The reply to the user:

        BRAINSTORMING mode:
        REFINE mode:

    * An explanation of any changes made to (what and why):

        - The current image generation model
        - The CFG parameter
        - The steps parameter

    NEXT: Implement above and implement the code that reacts to model
     or parameter change suggestions from the LLM.

    * Create log file of full session with user.  Assign a per session ID
     to a thread.  Implement the logging.  Write the session interaction
     to a text file.

- NEXT: Where and when do we actually make an image request?  The Chatbot
 app flow is more complicated than the Storytime app.  Call handleImageRequest()
 once that is determined.

- Lower stable diffusion steps during BRAINSTORM mode and higher in REFINE mode.
- Add the general negative prompts from storytime repo
- Use sharp NPM package to optimize images before returning them to the client
- Copy the optimized images to our S3 bucket
- Use S3 bucket URLs for NFT asset URLs.
- Create Amazon S3 bucket for generated images for the contest.  Later we can use a decentralized solution.
-